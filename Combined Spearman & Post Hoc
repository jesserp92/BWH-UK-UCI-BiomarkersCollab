#Load the following libraries to run in R

# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 12) # suppress math annotation
# load packages
library(boot)
library(broom)
library(car)
library(caret)
library(corrplot)
library(cowplot)
library(dplyr)
library(effects)
library(emmeans)
library(FSA)
library(flextable)
library(foreign)
library(GGally)
library(ggplot2)
library(ggfortify)
library(ggpubr)
library(gridExtra)
library(here)
library(Hmisc)
library(knitr)
library(lme4)
library(MASS)
library(mclogit)
library(mlogit)
library(msm)
library(MuMIn)
library(nlme)
library(ordinal)
library(patchwork)
library(performance)
library(PerformanceAnalytics)
library(QuantPsyc)
library(readxl)
library(remotes)
library(report)
library(reshape2)
library(rms)
library(robustbase)
library(sandwich)
library(sjPlot)
library(tibble)
library(tidyverse)
library(vcd)
library(vip)
library(visreg)
# activate klippy for copy-to-clipboard button
klippy::klippy()

#Read the file in the following format: "Titleofdocument.csv". From here on, the data will be abbreviated to "PL". 
PL <- read.csv("GMLoadsvsPlasma.csv", header = TRUE)
print(PL)
PL <- tibble::as_tibble(PL)
library(reshape2)

#Creating a correlation Matrix 
#[,unlist] step is necessary to make sure the only columns read are the ones with numerics. Any column with alphas or non-numerics will read as an error. 
#The following selects a subset of all numeric variables in our data using the unlist, lapply, and is.numeric functions as shown below:
PL.cor = cor(PL[, unlist(lapply(PL, is.numeric))], method = c("spearman"))  # Properly apply cor

#flattenCorrelationMatrix
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

#[,unlist] step is necessary to make sure the only columns read are the ones with numerics. Any column with alphas or non-numerics will read as an error. 
#The following runs a correlation matrix with p-values. 
PL.rcorr=rcorr(as.matrix(PL)[, unlist(lapply(PL, is.numeric))])
PL.rcorr
flattenCorrMatrix(PL.rcorr$r,PL.rcorr$P)

#Next we are going to generate one table of correlation coefficients and one with p-values. 
PL.coeff = PL.rcorr$r
PL.p = PL.rcorr$P

#Create correlation matrix based on the information generated above. 
corrplot(PL.cor)

#Create a correlogram to better visualize the graphical display
corrplot(PL.cor, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

#Combine correlogram with significance test

# Insignificant correlation are crossed
corrplot(PL.coeff, type="upper", order="hclust", 
         p.mat = PL.p, sig.level = 0.04, insig = "blank")
# Insignificant correlations are leaved blank
corrplot(PL.coeff, type="upper", order="hclust", 
         p.mat = PL.p, sig.level = 0.04, insig = "blank")

#now to display a chart of a correlation matrix
chart.Correlation(PL, histogram=TRUE, pch=19)

#Begin regression analysis by generating a regression model and inspecting results. 
#AB40 summary stats
ab40.lm<-lm(ab40plasma~AB40Loads, data = PL)

summary(ab40.lm)

#AB42 summary stats
ab42.lm<-lm(ab42plasma~AB42Loads, data = PL)

summary(ab42.lm)

#AB37 summary stats
ab37.lm<-lm(ab37plasma~AB37Loads, data = PL)

summary(ab37.lm)

#BT2Tau summary stats
nt1.lm<-lm(nt1plasma~BT2Loads, data = PL)

summary(nt1.lm)

#BT2TauRpt summary stats
nt1rpt.lm<-lm(nt1plasma~BT2LoadsRpt, data = PL)

summary(nt1rpt.lm)

#AT8Tau summary stats
at8tau.lm<-lm(nt1plasma~AT8Loads, data = PL)

summary(at8tau.lm)

#AT8TauRpt summary stats
at8rpt.lm<-lm(nt1plasma~AT8LoadsRpt, data = PL)

summary(at8rpt.lm)

#pS416 summary stats
ps416.lm<-lm(nt1plasma~pS416Loads, data = PL)

summary(ps416.lm)

#pT217 summary stats
pt217.lm<-lm(nt1plasma~pT217Loads, data = PL)

summary(pt217.lm)

#Check if the model for NT1 tau is a good fit based on residuals vs leverage and vs fitted
par(mfrow=c(2,2))
plot(nt1.lm)
par(mfrow=c(1,1))

#Check if the model for ab42 is a good fit based on residuals vs leverage and vs fitted
par(mfrow=c(2,2))
plot(ab42.lm)
par(mfrow=c(1,1))

#Check if the model for ab40 is a good fit based on residuals vs leverage and vs fitted
par(mfrow=c(2,2))
plot(ab40.lm)
par(mfrow=c(1,1))

#Check if the model for ab37 is a good fit based on residuals vs leverage and vs fitted
par(mfrow=c(2,2))
plot(ab37.lm)
par(mfrow=c(1,1))

#The following lines of code will focus on diagnostic modeling of the regression analysis and see if there are any influential points that may pull the data for NT1 tau plasma and BT2 tau for postmortem.
# generate data for NT1 tau
df2 <- data.frame(id = 1:length(resid(nt1.lm)),
                  residuals = resid(nt1.lm),
                  standard = rstandard(nt1.lm),
                  studend = rstudent(nt1.lm))
# generate plots
p1 <- ggplot(df2, aes(x = id, y = residuals)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Residuals", x = "Index")
p2 <- ggplot(df2, aes(x = id, y = standard)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Standardized Residuals", x = "Index")
p3 <- ggplot(df2, aes(x = id, y = studend)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Studentized Residuals", x = "Index")
# display plots
ggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)

#Generate graph for leverages for nt1
nt1tau.lm <- lm(nt1plasma~BT2Loads, data=PL)

# generate plots for nt1
autoplot(nt1tau.lm) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 

# determine a cutoff for data points that have
# D-values higher than 4/(n-k-1)
cutoff <- 4/((nrow(nt1tau.lm)-length(nt1tau.lm$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row/2 columns
qqPlot(nt1tau.lm, main="QQ Plot") # create qq-plot

plot(nt1tau.lm, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))

# add model diagnostics to the data for nt1
PL <- PL %>%
  dplyr::mutate(residuals = resid(nt1tau.lm),
                standardized.residuals = rstandard(nt1tau.lm),
                studentized.residuals = rstudent(nt1tau.lm),
                cooks.distance = cooks.distance(nt1tau.lm),
                dffit = dffits(nt1tau.lm),
                leverage = hatvalues(nt1tau.lm),
                covariance.ratios = covratio(nt1tau.lm),
                fitted = nt1tau.lm$fitted.values)
# plot 5
p5 <- ggplot(PL,
             aes(studentized.residuals)) +
  theme(legend.position = "none")+
  geom_histogram(aes(y=..density..),
                 binwidth = .2,
                 colour="black",
                 fill="gray90") +
  labs(x = "Studentized Residual", y = "Density") +
  stat_function(fun = dnorm,
                args = list(mean = mean(PL$studentized.residuals, na.rm = TRUE),
                            sd = sd(PL$studentized.residuals, na.rm = TRUE)),
                colour = "red", size = 1) +
  theme_bw(base_size = 8)
# plot 6
p6 <- ggplot(PL, aes(fitted, studentized.residuals)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "Red")+
  theme_bw(base_size = 8)+
  labs(x = "Fitted Values",
       y = "Studentized Residual")
# plot 7
p7 <- qplot(sample = PL$studentized.residuals, stat="qq") +
  theme_bw(base_size = 8) +
  labs(x = "Theoretical Values",
       y = "Observed Values")
vip::grid.arrange(p5, p6, p7, nrow = 1)

#Optional try checking if this is necessary

# 1: optimal = 0
# (listed data points should be removed)
which(nt1tau.lm$standardized.residuals > 3.29)

# 2: optimal = 1
# (listed data points should be removed)
stdres_258 <- as.vector(sapply(nt1tau.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100

# 3: optimal = 5
# (listed data points should be removed)
stdres_196 <- as.vector(sapply(nt1tau.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100

# 4: optimal = 0
# (listed data points should be removed)
which(nt1tau.lm$cooks.distance > 1)

# 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(nt1tau.lm$leverage >= (3*mean(nt1tau.lm$leverage)))

# 6: checking autocorrelation:
# Durbin-Watson test (optimal: high p-value)
dwt(nt1tau.lm)

# load functions
source("https://slcladal.github.io/rscripts/SampleSizeMLR.r")
source("https://slcladal.github.io/rscripts/ExpR.r")
# check if sample size is sufficient
smplesz(nt1tau.lm)

# check beta-error likelihood
expR(nt1tau.lm)

#The following lines of code will focus on diagnostic modeling of the regression analysis and see if there are any influential points that may pull the data for antemortem plasma and antibody loads postmortem.
# generate data for Bamy42
df2 <- data.frame(id = 1:length(resid(ab42.lm)),
                  residuals = resid(ab42.lm),
                  standard = rstandard(ab42.lm),
                  studend = rstudent(ab42.lm))
# generate plots
p1 <- ggplot(df2, aes(x = id, y = residuals)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Residuals", x = "Index")
p2 <- ggplot(df2, aes(x = id, y = standard)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Standardized Residuals", x = "Index")
p3 <- ggplot(df2, aes(x = id, y = studend)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Studentized Residuals", x = "Index")
# display plots
ggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)

#Generate graph for leverages for ab42
bamy42.lm <- lm(ab42plasma~AB42Loads, data=PL)

# generate plots for ab42
autoplot(bamy42.lm) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 

# determine a cutoff for data points that have
# D-values higher than 4/(n-k-1)
cutoff <- 4/((nrow(bamy42.lm)-length(bamy42.lm$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row/2 columns
qqPlot(bamy42.lm, main="QQ Plot") # create qq-plot

plot(bamy42.lm, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))

# add model diagnostics to the data for ab42
PL <- PL %>%
  dplyr::mutate(residuals = resid(bamy42.lm),
                standardized.residuals = rstandard(bamy42.lm),
                studentized.residuals = rstudent(bamy42.lm),
                cooks.distance = cooks.distance(bamy42.lm),
                dffit = dffits(bamy42.lm),
                leverage = hatvalues(bamy42.lm),
                covariance.ratios = covratio(bamy42.lm),
                fitted = bamy42.lm$fitted.values)
# plot 5
p5 <- ggplot(PL,
             aes(studentized.residuals)) +
  theme(legend.position = "none")+
  geom_histogram(aes(y=..density..),
                 binwidth = .2,
                 colour="black",
                 fill="gray90") +
  labs(x = "Studentized Residual", y = "Density") +
  stat_function(fun = dnorm,
                args = list(mean = mean(PL$studentized.residuals, na.rm = TRUE),
                            sd = sd(PL$studentized.residuals, na.rm = TRUE)),
                colour = "red", size = 1) +
  theme_bw(base_size = 8)
# plot 6
p6 <- ggplot(PL, aes(fitted, studentized.residuals)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "Red")+
  theme_bw(base_size = 8)+
  labs(x = "Fitted Values",
       y = "Studentized Residual")
# plot 7
p7 <- qplot(sample = PL$studentized.residuals, stat="qq") +
  theme_bw(base_size = 8) +
  labs(x = "Theoretical Values",
       y = "Observed Values")
vip::grid.arrange(p5, p6, p7, nrow = 1)

#Optional try checking if this is necessary

# 1: optimal = 0
# (listed data points should be removed)
which(bamy42.lm$standardized.residuals > 3.29)

# 2: optimal = 1
# (listed data points should be removed)
stdres_258 <- as.vector(sapply(bamy42.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100

# 3: optimal = 5
# (listed data points should be removed)
stdres_196 <- as.vector(sapply(bamy42.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100

# 4: optimal = 0
# (listed data points should be removed)
which(bamy42.lm$cooks.distance > 1)

# 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(bamy42.lm$leverage >= (3*mean(bamy42.lm$leverage)))

# 6: checking autocorrelation:
# Durbin-Watson test (optimal: high p-value)
dwt(bamy42.lm)

# load functions
source("https://slcladal.github.io/rscripts/SampleSizeMLR.r")
source("https://slcladal.github.io/rscripts/ExpR.r")
# check if sample size is sufficient
smplesz(bamy42.lm)

# check beta-error likelihood
expR(bamy42.lm)

#The following lines of code will focus on diagnostic modeling of the regression analysis and see if there are any influential points that may pull the data for antemortem plasma and antibody loads postmortem.
# generate data for Bamy 40
df2 <- data.frame(id = 1:length(resid(ab40.lm)),
                  residuals = resid(ab40.lm),
                  standard = rstandard(ab40.lm),
                  studend = rstudent(ab40.lm))
# generate plots
p1 <- ggplot(df2, aes(x = id, y = residuals)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Residuals", x = "Index")
p2 <- ggplot(df2, aes(x = id, y = standard)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Standardized Residuals", x = "Index")
p3 <- ggplot(df2, aes(x = id, y = studend)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Studentized Residuals", x = "Index")
# display plots
ggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)

#Generate graph for leverages for ab40
bamy40.lm <- lm(ab40plasma~AB40Loads, data=PL)

# generate plots for ab40
autoplot(bamy40.lm ) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 

# determine a cutoff for data points that have
# D-values higher than 4/(n-k-1)
cutoff <- 4/((nrow(bamy40.lm )-length(bamy40.lm $coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row/2 columns
qqPlot(bamy40.lm , main="QQ Plot") # create qq-plot

plot(bamy40.lm , which=4, cook.levels = cutoff); par(mfrow = c(1, 1))

# add model diagnostics to the data for ab40
PL <- PL %>%
  dplyr::mutate(residuals = resid(bamy40.lm ),
                standardized.residuals = rstandard(bamy40.lm ),
                studentized.residuals = rstudent(bamy40.lm ),
                cooks.distance = cooks.distance(bamy40.lm ),
                dffit = dffits(bamy40.lm ),
                leverage = hatvalues(bamy40.lm ),
                covariance.ratios = covratio(bamy40.lm ),
                fitted = bamy40.lm $fitted.values)
# plot 5
p5 <- ggplot(PL,
             aes(studentized.residuals)) +
  theme(legend.position = "none")+
  geom_histogram(aes(y=..density..),
                 binwidth = .2,
                 colour="black",
                 fill="gray90") +
  labs(x = "Studentized Residual", y = "Density") +
  stat_function(fun = dnorm,
                args = list(mean = mean(PL$studentized.residuals, na.rm = TRUE),
                            sd = sd(PL$studentized.residuals, na.rm = TRUE)),
                colour = "red", size = 1) +
  theme_bw(base_size = 8)
# plot 6
p6 <- ggplot(PL, aes(fitted, studentized.residuals)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "Red")+
  theme_bw(base_size = 8)+
  labs(x = "Fitted Values",
       y = "Studentized Residual")
# plot 7
p7 <- qplot(sample = PL$studentized.residuals, stat="qq") +
  theme_bw(base_size = 8) +
  labs(x = "Theoretical Values",
       y = "Observed Values")
vip::grid.arrange(p5, p6, p7, nrow = 1)

#Optional try checking if this is necessary

# 1: optimal = 0
# (listed data points should be removed)
which(bamy40.lm $standardized.residuals > 3.29)

# 2: optimal = 1
# (listed data points should be removed)
stdres_258 <- as.vector(sapply(bamy40.lm $standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100

# 3: optimal = 5
# (listed data points should be removed)
stdres_196 <- as.vector(sapply(bamy40.lm $standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100

# 4: optimal = 0
# (listed data points should be removed)
which(bamy40.lm $cooks.distance > 1)

# 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(bamy40.lm $leverage >= (3*mean(bamy40.lm $leverage)))

# 6: checking autocorrelation:
# Durbin-Watson test (optimal: high p-value)
dwt(bamy40.lm )

# load functions
source("https://slcladal.github.io/rscripts/SampleSizeMLR.r")
source("https://slcladal.github.io/rscripts/ExpR.r")
# check if sample size is sufficient
smplesz(bamy40.lm )

# check beta-error likelihood
expR(bamy40.lm )

#The following lines of code will focus on diagnostic modeling of the regression analysis and see if there are any influential points that may pull the data for antemortem plasma and antibody loads postmortem.
# generate data for ab37
df2 <- data.frame(id = 1:length(resid(ab37.lm)),
                  residuals = resid(ab37.lm),
                  standard = rstandard(ab37.lm),
                  studend = rstudent(ab37.lm))
# generate plots
p1 <- ggplot(df2, aes(x = id, y = residuals)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Residuals", x = "Index")
p2 <- ggplot(df2, aes(x = id, y = standard)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Standardized Residuals", x = "Index")
p3 <- ggplot(df2, aes(x = id, y = studend)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Studentized Residuals", x = "Index")
# display plots
ggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)

#Generate graph for leverages for nt1
bamy37.lm <- lm(ab37plasma~AB37Loads, data=PL)

# generate plots for nt1
autoplot(bamy37.lm) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 

# determine a cutoff for data points that have
# D-values higher than 4/(n-k-1)
cutoff <- 4/((nrow(bamy37.lm)-length(bamy37.lm$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row/2 columns
qqPlot(bamy37.lm, main="QQ Plot") # create qq-plot

plot(bamy37.lm, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))

# add model diagnostics to the data for nt1
PL <- PL %>%
  dplyr::mutate(residuals = resid(bamy37.lm),
                standardized.residuals = rstandard(bamy37.lm),
                studentized.residuals = rstudent(bamy37.lm),
                cooks.distance = cooks.distance(bamy37.lm),
                dffit = dffits(bamy37.lm),
                leverage = hatvalues(bamy37.lm),
                covariance.ratios = covratio(bamy37.lm),
                fitted = bamy37.lm$fitted.values)
# plot 5
p5 <- ggplot(PL,
             aes(studentized.residuals)) +
  theme(legend.position = "none")+
  geom_histogram(aes(y=..density..),
                 binwidth = .2,
                 colour="black",
                 fill="gray90") +
  labs(x = "Studentized Residual", y = "Density") +
  stat_function(fun = dnorm,
                args = list(mean = mean(PL$studentized.residuals, na.rm = TRUE),
                            sd = sd(PL$studentized.residuals, na.rm = TRUE)),
                colour = "red", size = 1) +
  theme_bw(base_size = 8)
# plot 6
p6 <- ggplot(PL, aes(fitted, studentized.residuals)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "Red")+
  theme_bw(base_size = 8)+
  labs(x = "Fitted Values",
       y = "Studentized Residual")
# plot 7
p7 <- qplot(sample = PL$studentized.residuals, stat="qq") +
  theme_bw(base_size = 8) +
  labs(x = "Theoretical Values",
       y = "Observed Values")
vip::grid.arrange(p5, p6, p7, nrow = 1)

#Optional try checking if this is necessary

# 1: optimal = 0
# (listed data points should be removed)
which(bamy37.lm$standardized.residuals > 3.29)

# 2: optimal = 1
# (listed data points should be removed)
stdres_258 <- as.vector(sapply(bamy37.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100

# 3: optimal = 5
# (listed data points should be removed)
stdres_196 <- as.vector(sapply(bamy37.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100

# 4: optimal = 0
# (listed data points should be removed)
which(bamy37.lm$cooks.distance > 1)

# 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(bamy37.lm$leverage >= (3*mean(bamy37.lm$leverage)))

# 6: checking autocorrelation:
# Durbin-Watson test (optimal: high p-value)
dwt(bamy37.lm)

# load functions
source("https://slcladal.github.io/rscripts/SampleSizeMLR.r")
source("https://slcladal.github.io/rscripts/ExpR.r")
# check if sample size is sufficient
smplesz(bamy37.lm)

# check beta-error likelihood
expR(bamy37.lm)

#The following lines of code will focus on diagnostic modeling of the regression analysis and see if there are any influential points that may pull the data for NT1 tau plasma and BT2 tau for postmortem.
# generate data for NT1 tau rpt
df2 <- data.frame(id = 1:length(resid(nt1rpt.lm)),
                  residuals = resid(nt1rpt.lm),
                  standard = rstandard(nt1rpt.lm),
                  studend = rstudent(nt1rpt.lm))
# generate plots
p1 <- ggplot(df2, aes(x = id, y = residuals)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Residuals", x = "Index")
p2 <- ggplot(df2, aes(x = id, y = standard)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Standardized Residuals", x = "Index")
p3 <- ggplot(df2, aes(x = id, y = studend)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Studentized Residuals", x = "Index")
# display plots
ggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)

#Generate graph for leverages for nt1
nt1taurpt.lm <- lm(nt1plasma~BT2LoadsRpt, data=PL)

# generate plots for nt1rpt
autoplot(nt1taurpt.lm) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 

# determine a cutoff for data points that have
# D-values higher than 4/(n-k-1)
cutoff <- 4/((nrow(nt1taurpt.lm)-length(nt1taurpt.lm$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row/2 columns
qqPlot(nt1taurpt.lm, main="QQ Plot") # create qq-plot

plot(nt1taurpt.lm, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))

# add model diagnostics to the data for nt1
PL <- PL %>%
  dplyr::mutate(residuals = resid(nt1taurpt.lm),
                standardized.residuals = rstandard(nt1taurpt.lm),
                studentized.residuals = rstudent(nt1taurpt.lm),
                cooks.distance = cooks.distance(nt1taurpt.lm),
                dffit = dffits(nt1taurpt.lm),
                leverage = hatvalues(nt1taurpt.lm),
                covariance.ratios = covratio(nt1taurpt.lm),
                fitted = nt1taurpt.lm$fitted.values)
# plot 5
p5 <- ggplot(PL,
             aes(studentized.residuals)) +
  theme(legend.position = "none")+
  geom_histogram(aes(y=..density..),
                 binwidth = .2,
                 colour="black",
                 fill="gray90") +
  labs(x = "Studentized Residual", y = "Density") +
  stat_function(fun = dnorm,
                args = list(mean = mean(PL$studentized.residuals, na.rm = TRUE),
                            sd = sd(PL$studentized.residuals, na.rm = TRUE)),
                colour = "red", size = 1) +
  theme_bw(base_size = 8)
# plot 6
p6 <- ggplot(PL, aes(fitted, studentized.residuals)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "Red")+
  theme_bw(base_size = 8)+
  labs(x = "Fitted Values",
       y = "Studentized Residual")
# plot 7
p7 <- qplot(sample = PL$studentized.residuals, stat="qq") +
  theme_bw(base_size = 8) +
  labs(x = "Theoretical Values",
       y = "Observed Values")
vip::grid.arrange(p5, p6, p7, nrow = 1)

#Optional try checking if this is necessary

# 1: optimal = 0
# (listed data points should be removed)
which(nt1taurpt.lm$standardized.residuals > 3.29)

# 2: optimal = 1
# (listed data points should be removed)
stdres_258 <- as.vector(sapply(nt1taurpt.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100

# 3: optimal = 5
# (listed data points should be removed)
stdres_196 <- as.vector(sapply(nt1taurpt.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100

# 4: optimal = 0
# (listed data points should be removed)
which(nt1taurpt.lm$cooks.distance > 1)

# 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(nt1taurpt.lm$leverage >= (3*mean(nt1taurpt.lm$leverage)))

# 6: checking autocorrelation:
# Durbin-Watson test (optimal: high p-value)
dwt(nt1taurpt.lm)

# load functions
source("https://slcladal.github.io/rscripts/SampleSizeMLR.r")
source("https://slcladal.github.io/rscripts/ExpR.r")
# check if sample size is sufficient
smplesz(nt1taurpt.lm)

# check beta-error likelihood
expR(nt1taurpt.lm)

#The following lines of code will focus on diagnostic modeling of the regression analysis and see if there are any influential points that may pull the data for NT1 tau plasma and BT2 tau for postmortem.
# generate data for AT8 tau
df2 <- data.frame(id = 1:length(resid(at8tau.lm)),
                  residuals = resid(at8tau.lm),
                  standard = rstandard(at8tau.lm),
                  studend = rstudent(at8tau.lm))
# generate plots
p1 <- ggplot(df2, aes(x = id, y = residuals)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Residuals", x = "Index")
p2 <- ggplot(df2, aes(x = id, y = standard)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Standardized Residuals", x = "Index")
p3 <- ggplot(df2, aes(x = id, y = studend)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Studentized Residuals", x = "Index")
# display plots
ggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)

#Generate graph for leverages for at8
at8tau.lm <- lm(nt1plasma~AT8Loads, data=PL)

# generate plots for at8
autoplot(at8tau.lm) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 

# determine a cutoff for data points that have
# D-values higher than 4/(n-k-1)
cutoff <- 4/((nrow(at8tau.lm)-length(at8tau.lm$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row/2 columns
qqPlot(at8tau.lm, main="QQ Plot") # create qq-plot

plot(at8tau.lm, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))

# add model diagnostics to the data for at8
PL <- PL %>%
  dplyr::mutate(residuals = resid(at8tau.lm),
                standardized.residuals = rstandard(at8tau.lm),
                studentized.residuals = rstudent(at8tau.lm),
                cooks.distance = cooks.distance(at8tau.lm),
                dffit = dffits(at8tau.lm),
                leverage = hatvalues(at8tau.lm),
                covariance.ratios = covratio(at8tau.lm),
                fitted = at8tau.lm$fitted.values)
# plot 5
p5 <- ggplot(PL,
             aes(studentized.residuals)) +
  theme(legend.position = "none")+
  geom_histogram(aes(y=..density..),
                 binwidth = .2,
                 colour="black",
                 fill="gray90") +
  labs(x = "Studentized Residual", y = "Density") +
  stat_function(fun = dnorm,
                args = list(mean = mean(PL$studentized.residuals, na.rm = TRUE),
                            sd = sd(PL$studentized.residuals, na.rm = TRUE)),
                colour = "red", size = 1) +
  theme_bw(base_size = 8)
# plot 6
p6 <- ggplot(PL, aes(fitted, studentized.residuals)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "Red")+
  theme_bw(base_size = 8)+
  labs(x = "Fitted Values",
       y = "Studentized Residual")
# plot 7
p7 <- qplot(sample = PL$studentized.residuals, stat="qq") +
  theme_bw(base_size = 8) +
  labs(x = "Theoretical Values",
       y = "Observed Values")
vip::grid.arrange(p5, p6, p7, nrow = 1)

#Optional try checking if this is necessary

# 1: optimal = 0
# (listed data points should be removed)
which(at8tau.lm$standardized.residuals > 3.29)

# 2: optimal = 1
# (listed data points should be removed)
stdres_258 <- as.vector(sapply(at8tau.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100

# 3: optimal = 5
# (listed data points should be removed)
stdres_196 <- as.vector(sapply(at8tau.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100

# 4: optimal = 0
# (listed data points should be removed)
which(at8tau.lm$cooks.distance > 1)

# 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(at8tau.lm$leverage >= (3*mean(at8tau.lm$leverage)))

# 6: checking autocorrelation:
# Durbin-Watson test (optimal: high p-value)
dwt(at8tau.lm)

# load functions
source("https://slcladal.github.io/rscripts/SampleSizeMLR.r")
source("https://slcladal.github.io/rscripts/ExpR.r")
# check if sample size is sufficient
smplesz(at8tau.lm)

# check beta-error likelihood
expR(at8tau.lm)

#The following lines of code will focus on diagnostic modeling of the regression analysis and see if there are any influential points that may pull the data for NT1 tau plasma and BT2 tau for postmortem.
# generate data for AT8rpt tau
df2 <- data.frame(id = 1:length(resid(at8rpt.lm)),
                  residuals = resid(at8rpt.lm),
                  standard = rstandard(at8rpt.lm),
                  studend = rstudent(at8rpt.lm))
# generate plots
p1 <- ggplot(df2, aes(x = id, y = residuals)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Residuals", x = "Index")
p2 <- ggplot(df2, aes(x = id, y = standard)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Standardized Residuals", x = "Index")
p3 <- ggplot(df2, aes(x = id, y = studend)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Studentized Residuals", x = "Index")
# display plots
ggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)

#Generate graph for leverages for at8 rpt
at8taurpt.lm <- lm(nt1plasma~AT8LoadsRpt, data=PL)

# generate plots for at8 rpt
autoplot(at8taurpt.lm) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 

# determine a cutoff for data points that have
# D-values higher than 4/(n-k-1)
cutoff <- 4/((nrow(at8taurpt.lm)-length(at8taurpt.lm$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row/2 columns
qqPlot(at8taurpt.lm, main="QQ Plot") # create qq-plot

plot(at8taurpt.lm, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))

# add model diagnostics to the data for at8 rpt
PL <- PL %>%
  dplyr::mutate(residuals = resid(at8taurpt.lm),
                standardized.residuals = rstandard(at8taurpt.lm),
                studentized.residuals = rstudent(at8taurpt.lm),
                cooks.distance = cooks.distance(at8taurpt.lm),
                dffit = dffits(at8taurpt.lm),
                leverage = hatvalues(at8taurpt.lm),
                covariance.ratios = covratio(at8taurpt.lm),
                fitted = at8taurpt.lm$fitted.values)
# plot 5
p5 <- ggplot(PL,
             aes(studentized.residuals)) +
  theme(legend.position = "none")+
  geom_histogram(aes(y=..density..),
                 binwidth = .2,
                 colour="black",
                 fill="gray90") +
  labs(x = "Studentized Residual", y = "Density") +
  stat_function(fun = dnorm,
                args = list(mean = mean(PL$studentized.residuals, na.rm = TRUE),
                            sd = sd(PL$studentized.residuals, na.rm = TRUE)),
                colour = "red", size = 1) +
  theme_bw(base_size = 8)
# plot 6
p6 <- ggplot(PL, aes(fitted, studentized.residuals)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "Red")+
  theme_bw(base_size = 8)+
  labs(x = "Fitted Values",
       y = "Studentized Residual")
# plot 7
p7 <- qplot(sample = PL$studentized.residuals, stat="qq") +
  theme_bw(base_size = 8) +
  labs(x = "Theoretical Values",
       y = "Observed Values")
vip::grid.arrange(p5, p6, p7, nrow = 1)

#Optional try checking if this is necessary

# 1: optimal = 0
# (listed data points should be removed)
which(at8taurpt.lm$standardized.residuals > 3.29)

# 2: optimal = 1
# (listed data points should be removed)
stdres_258 <- as.vector(sapply(at8taurpt.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100

# 3: optimal = 5
# (listed data points should be removed)
stdres_196 <- as.vector(sapply(at8taurpt.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100

# 4: optimal = 0
# (listed data points should be removed)
which(at8taurpt.lm$cooks.distance > 1)

# 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(at8taurpt.lm$leverage >= (3*mean(at8taurpt.lm$leverage)))

# 6: checking autocorrelation:
# Durbin-Watson test (optimal: high p-value)
dwt(at8taurpt.lm)

# load functions
source("https://slcladal.github.io/rscripts/SampleSizeMLR.r")
source("https://slcladal.github.io/rscripts/ExpR.r")
# check if sample size is sufficient
smplesz(at8taurpt.lm)

# check beta-error likelihood
expR(at8taurpt.lm)

#The following lines of code will focus on diagnostic modeling of the regression analysis and see if there are any influential points that may pull the data for NT1 tau plasma and BT2 tau for postmortem.
# generate data for pS416 tau
df2 <- data.frame(id = 1:length(resid(ps416.lm)),
                  residuals = resid(ps416.lm),
                  standard = rstandard(ps416.lm),
                  studend = rstudent(ps416.lm))
# generate plots
p1 <- ggplot(df2, aes(x = id, y = residuals)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Residuals", x = "Index")
p2 <- ggplot(df2, aes(x = id, y = standard)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Standardized Residuals", x = "Index")
p3 <- ggplot(df2, aes(x = id, y = studend)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Studentized Residuals", x = "Index")
# display plots
ggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)

#Generate graph for leverages for ps416 
pS416tau.lm <- lm(nt1plasma~pS416Loads, data=PL)

# generate plots for ps416 
autoplot(pS416tau.lm) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 

# determine a cutoff for data points that have
# D-values higher than 4/(n-k-1)
cutoff <- 4/((nrow(pS416tau.lm)-length(pS416tau.lm$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row/2 columns
qqPlot(pS416tau.lm, main="QQ Plot") # create qq-plot

plot(pS416tau.lm, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))

# add model diagnostics to the data for ps416 
PL <- PL %>%
  dplyr::mutate(residuals = resid(pS416tau.lm),
                standardized.residuals = rstandard(pS416tau.lm),
                studentized.residuals = rstudent(pS416tau.lm),
                cooks.distance = cooks.distance(pS416tau.lm),
                dffit = dffits(pS416tau.lm),
                leverage = hatvalues(pS416tau.lm),
                covariance.ratios = covratio(pS416tau.lm),
                fitted = pS416tau.lm$fitted.values)
# plot 5
p5 <- ggplot(PL,
             aes(studentized.residuals)) +
  theme(legend.position = "none")+
  geom_histogram(aes(y=..density..),
                 binwidth = .2,
                 colour="black",
                 fill="gray90") +
  labs(x = "Studentized Residual", y = "Density") +
  stat_function(fun = dnorm,
                args = list(mean = mean(PL$studentized.residuals, na.rm = TRUE),
                            sd = sd(PL$studentized.residuals, na.rm = TRUE)),
                colour = "red", size = 1) +
  theme_bw(base_size = 8)
# plot 6
p6 <- ggplot(PL, aes(fitted, studentized.residuals)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "Red")+
  theme_bw(base_size = 8)+
  labs(x = "Fitted Values",
       y = "Studentized Residual")
# plot 7
p7 <- qplot(sample = PL$studentized.residuals, stat="qq") +
  theme_bw(base_size = 8) +
  labs(x = "Theoretical Values",
       y = "Observed Values")
vip::grid.arrange(p5, p6, p7, nrow = 1)

#Optional try checking if this is necessary

# 1: optimal = 0
# (listed data points should be removed)
which(pS416tau.lm$standardized.residuals > 3.29)

# 2: optimal = 1
# (listed data points should be removed)
stdres_258 <- as.vector(sapply(pS416tau.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100

# 3: optimal = 5
# (listed data points should be removed)
stdres_196 <- as.vector(sapply(pS416tau.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100

# 4: optimal = 0
# (listed data points should be removed)
which(pS416tau.lm$cooks.distance > 1)

# 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(pS416tau.lm$leverage >= (3*mean(pS416tau.lm$leverage)))

# 6: checking autocorrelation:
# Durbin-Watson test (optimal: high p-value)
dwt(pS416tau.lm)

# load functions
source("https://slcladal.github.io/rscripts/SampleSizeMLR.r")
source("https://slcladal.github.io/rscripts/ExpR.r")
# check if sample size is sufficient
smplesz(pS416tau.lm)

# check beta-error likelihood
expR(pS416tau.lm)

#The following lines of code will focus on diagnostic modeling of the regression analysis and see if there are any influential points that may pull the data for NT1 tau plasma and BT2 tau for postmortem.
# generate data for pT217 tau
df2 <- data.frame(id = 1:length(resid(pt217.lm)),
                  residuals = resid(pt217.lm),
                  standard = rstandard(pt217.lm),
                  studend = rstudent(pt217.lm))
# generate plots
p1 <- ggplot(df2, aes(x = id, y = residuals)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Residuals", x = "Index")
p2 <- ggplot(df2, aes(x = id, y = standard)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Standardized Residuals", x = "Index")
p3 <- ggplot(df2, aes(x = id, y = studend)) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  geom_point() +
  labs(y = "Studentized Residuals", x = "Index")
# display plots
ggpubr::ggarrange(p1, p2, p3, ncol = 3, nrow = 1)

#Generate graph for leverages for pT217 
pT217tau.lm <- lm(nt1plasma~pT217Loads, data=PL)

# generate plots for pT217 
autoplot(pT217tau.lm) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 

# determine a cutoff for data points that have
# D-values higher than 4/(n-k-1)
cutoff <- 4/((nrow(pT217tau.lm)-length(pT217tau.lm$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row/2 columns
qqPlot(pT217tau.lm, main="QQ Plot") # create qq-plot

plot(pT217tau.lm, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))

# add model diagnostics to the data for pT217 
PL <- PL %>%
  dplyr::mutate(residuals = resid(pT217tau.lm),
                standardized.residuals = rstandard(pT217tau.lm),
                studentized.residuals = rstudent(pT217tau.lm),
                cooks.distance = cooks.distance(pT217tau.lm),
                dffit = dffits(pT217tau.lm),
                leverage = hatvalues(pT217tau.lm),
                covariance.ratios = covratio(pT217tau.lm),
                fitted = pT217tau.lm$fitted.values)
# plot 5
p5 <- ggplot(PL,
             aes(studentized.residuals)) +
  theme(legend.position = "none")+
  geom_histogram(aes(y=..density..),
                 binwidth = .2,
                 colour="black",
                 fill="gray90") +
  labs(x = "Studentized Residual", y = "Density") +
  stat_function(fun = dnorm,
                args = list(mean = mean(PL$studentized.residuals, na.rm = TRUE),
                            sd = sd(PL$studentized.residuals, na.rm = TRUE)),
                colour = "red", size = 1) +
  theme_bw(base_size = 8)
# plot 6
p6 <- ggplot(PL, aes(fitted, studentized.residuals)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "Red")+
  theme_bw(base_size = 8)+
  labs(x = "Fitted Values",
       y = "Studentized Residual")
# plot 7
p7 <- qplot(sample = PL$studentized.residuals, stat="qq") +
  theme_bw(base_size = 8) +
  labs(x = "Theoretical Values",
       y = "Observed Values")
vip::grid.arrange(p5, p6, p7, nrow = 1)

#Optional try checking if this is necessary

# 1: optimal = 0
# (listed data points should be removed)
which(pT217tau.lm$standardized.residuals > 3.29)

# 2: optimal = 1
# (listed data points should be removed)
stdres_258 <- as.vector(sapply(pT217tau.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100

# 3: optimal = 5
# (listed data points should be removed)
stdres_196 <- as.vector(sapply(pT217tau.lm$standardized.residuals, function(x) {
  ifelse(sqrt((x^2)) > 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100

# 4: optimal = 0
# (listed data points should be removed)
which(pT217tau.lm$cooks.distance > 1)

# 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(pT217tau.lm$leverage >= (3*mean(pT217tau.lm$leverage)))

# 6: checking autocorrelation:
# Durbin-Watson test (optimal: high p-value)
dwt(pT217tau.lm)

# load functions
source("https://slcladal.github.io/rscripts/SampleSizeMLR.r")
source("https://slcladal.github.io/rscripts/ExpR.r")
# check if sample size is sufficient
smplesz(pT217tau.lm)

# check beta-error likelihood
expR(pT217tau.lm)

# tabulate model results
tab_model(nt1tau.lm, bamy42.lm, bamy40.lm, bamy37.lm, nt1taurpt.lm, at8tau.lm, at8taurpt.lm, pS416tau.lm, pT217tau.lm)

#The next lines of code will compare correlations between batches and a coefficient of variation for interbatch analysis of assays. 
# Scatter plot with correlation coefficient
#:::::::::::::::::::::::::::::::::::::::::::::::::
pct <- ggscatter(PL, x = "nt1plasma", y = "pT217Loads",
                 add = "reg.line",  # Add regression line
                 add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                 conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank for pT217Loads
pct + stat_cor(method = "spearman", label.x = 1, label.y = 4) +
  scale_y_continuous(trans='log2')


# Scatter plot with correlation coefficient
#:::::::::::::::::::::::::::::::::::::::::::::::::
pct <- ggscatter(PL, x = "nt1plasma", y = "pS416Loads",
                 add = "reg.line",  # Add regression line
                 add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                 conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank for pS416Loads
pct + stat_cor(method = "spearman", label.x = 1, label.y = 4) +
  scale_y_continuous(trans='log2')

# Scatter plot with correlation coefficient for BT2 Loads
#:::::::::::::::::::::::::::::::::::::::::::::::::
bt2batch <- ggscatter(PL, x = "BT2Loads", y = "BT2LoadsRpt",
                      add = "reg.line",  # Add regression line
                      add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                      conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank for BT2 vs BT2rpt
bt2batch + stat_cor(method = "spearman", label.x = 0.005, label.y = 0.01)+
  scale_y_continuous(trans='log2')

# Scatter plot with correlation coefficient for AT8 Loads
#:::::::::::::::::::::::::::::::::::::::::::::::::
at8batch <- ggscatter(PL, x = "AT8Loads", y = "AT8LoadsRpt",
                      add = "reg.line",  # Add regression line
                      add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                      conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank for AT8 vs AT8rpt
at8batch + stat_cor(method = "spearman", label.x = 0.1, label.y = 0.5)+
  scale_y_continuous(trans='log2')


#The next lines of code are dedicated to replotting the loads before comparing to plasma analytes using Spearman's Rank Correlation. 
#Create new dataframe to plot the model

plotting.data<-expand.grid(
  AB40Loads = seq(min(PL$AB40Loads), max(PL$AB40Loads)),
  AB42Loads=c(min(PL$AB42Loads), mean(PL$AB42Loads), max(PL$AB42Loads)),
  AB37Loads=c(min(PL$AB37Loads), mean(PL$AB37Loads), max(PL$AB37Loads)),
  BT2Loads=c(min(PL$BT2Loads), mean(PL$BT2Loads), max(PL$BT2Loads)),
  age=c(min(PL$age), mean(PL$age), max(PL$age)))

#Predict the values of NT1 based on new model
plotting.data$predicted.y <- predict.lm(nt1.lm, newdata=plotting.data)

#Round the values to two decimals
plotting.data$AB40Loads <- round(plotting.data$AB40Loads, digits = 3)
plotting.data$AB42Loads <- round(plotting.data$AB42Loads, digits = 3)
plotting.data$AB37Loads <- round(plotting.data$AB37Loads, digits = 3)
plotting.data$BT2Loads <- round(plotting.data$BT2Loads, digits = 3)

#Change age variable into a factor
plotting.data$age <- as.factor(plotting.data$age)

#Plot the graph again!
sp <- ggplot(PL, aes(age, nt1plasma)) + geom_point()+theme_classic()+ 
  geom_point(size = 4, shape = 21)

# Fit regression line
require(stats)
reg<-lm(nt1plasma ~ age, data = PL)
reg


#geom_abline : Add regression lines
coeff=coefficients(reg)
# Equation of the line : 
eq = paste0("y = ", round(coeff[2],1), "*x + ", round(coeff[1],1))
# Plot
sp + geom_abline(intercept = 7, slope = -0.03)+
  ggtitle(eq)
# Change line type, color and size
sp + geom_abline(intercept = 7, slope = -0.03, color="black", 
                 linetype="dashed", size=1.5)+
  ggtitle(eq)


# Scatter plot with correlation coefficient
#:::::::::::::::::::::::::::::::::::::::::::::::::
pct <- ggscatter(PL, x = "nt1plasma", y = "BT2Loads",
                 add = "reg.line",  # Add regression line
                 add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                 conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank for NT1 vs BT2
pct + stat_cor(method = "spearman", label.x = 1, label.y = 2)+
  scale_y_continuous(trans='log2')

# Scatter plot with correlation coefficient
#:::::::::::::::::::::::::::::::::::::::::::::::::
pct2 <- ggscatter(PL, x = "nt1plasma", y = "BT2LoadsRpt",
                 add = "reg.line",  # Add regression line
                 add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                 conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank for NT1 vs BT2rpt
pct2 + stat_cor(method = "spearman", label.x = 1, label.y = 2)+
  scale_y_continuous(trans='log2')

# Scatter plot with correlation coefficient
#:::::::::::::::::::::::::::::::::::::::::::::::::
at8 <- ggscatter(PL, x = "nt1plasma", y = "AT8Loads",
                  add = "reg.line",  # Add regression line
                  add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                  conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank for NT1 vs AT8
at8 + stat_cor(method = "spearman", label.x = 1, label.y = 2)+
  scale_y_continuous(trans='log2')


# Scatter plot with correlation coefficient
#:::::::::::::::::::::::::::::::::::::::::::::::::
at82 <- ggscatter(PL, x = "nt1plasma", y = "AT8LoadsRpt",
                  add = "reg.line",  # Add regression line
                  add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                  conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank for NT1 vs AT8rpt
at82 + stat_cor(method = "spearman", label.x = 1, label.y = 2)+
  scale_y_continuous(trans='log2')


#Plot the graph for ab40 plasma
sp <- ggplot(PL, aes(age, ab40plasma)) + geom_point()+theme_classic()+ 
  geom_point(size = 4, shape = 21)

#Create a regression for AB40
# Fit regression line
require(stats)
reg<-lm(ab40plasma ~ age, data = PL)
reg

#geom_abline : Add regression lines for AB40
coeff=coefficients(reg)
# Equation of the line : 
eq = paste0("y = ", round(coeff[2],1), "*x + ", round(coeff[1],1))
# Plot
sp + geom_abline(intercept = 110.368, slope = -0.282)+
  ggtitle(eq)
# Change line type, color and size
sp + geom_abline(intercept = 110.368, slope = -0.282, color="black", 
                 linetype="dashed", size=1.5)+
  ggtitle(eq)


# Scatter plot with correlation coefficient
#:::::::::::::::::::::::::::::::::::::::::::::::::
ab40sr <- ggscatter(PL, x = "ab40plasma", y = "AB40Loads",
                    add = "reg.line",  # Add regression line
                    add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                    conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank
ab40sr + stat_cor(method = "spearman", label.x = 10, label.y = 10)+
  scale_x_continuous(trans='log2')+
  scale_y_continuous(trans='log2')

#Plot the graph again for AB42!
sp <- ggplot(PL, aes(age, ab42plasma)) + geom_point()+theme_classic()+ 
  geom_point(size = 4, shape = 21)

#Create a regression for AB42
# Fit regression line
require(stats)
reg<-lm(ab42plasma ~ age, data = PL)
reg

#geom_abline : Add regression lines for AB42
coeff=coefficients(reg)
# Equation of the line : 
eq = paste0("y = ", round(coeff[2],1), "*x + ", round(coeff[1],1))
# Plot
sp + geom_abline(intercept = -1127, slope = 102.2)+
  ggtitle(eq)
# Change line type, color and size
sp + geom_abline(intercept = -1127, slope = 102.2, color="black", 
                 linetype="dashed", size=1.5)+
  ggtitle(eq)


# Scatter plot with correlation coefficient
#:::::::::::::::::::::::::::::::::::::::::::::::::
ab42sr <- ggscatter(PL, x = "ab42plasma", y = "AB42Loads",
                    add = "reg.line",  # Add regression line
                    add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                    conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank
ab42sr + stat_cor(method = "spearman", label.x = 100, label.y = 8)+
  scale_y_continuous(trans='log2')

#Plot the graph again for AB37!
sp <- ggplot(PL, aes(age, ab37plasma)) + geom_point()+theme_classic()+ 
  geom_point(size = 4, shape = 21)

#Create a regression for AB37
# Fit regression line
require(stats)
reg<-lm(ab37plasma ~ age, data = PL)
reg

#geom_abline : Add regression lines for AB37
coeff=coefficients(reg)
# Equation of the line : 
eq = paste0("y = ", round(coeff[2],1), "*x + ", round(coeff[1],1))
# Plot
sp + geom_abline(intercept = -176.751, slope = 6)+
  ggtitle(eq)
# Change line type, color and size
sp + geom_abline(intercept = -176.751, slope = 6, color="black", 
                 linetype="dashed", size=1.5)+
  ggtitle(eq)


# Scatter plot with correlation coefficient
#:::::::::::::::::::::::::::::::::::::::::::::::::
ab37sr <- ggscatter(PL, x = "ab37plasma", y = "AB37Loads",
                    add = "reg.line",  # Add regression line
                    add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                    conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank
ab37sr + stat_cor(method = "spearman", label.x = 8, label.y = 5)+
  scale_y_continuous(trans='log2')+
  scale_x_continuous(trans='log2')

#Plot all the spearman in one plot
plot_grid(pct, ab40sr, ab42sr, ab37sr, labels=c("A", "B", "C", "D"), ncol = 4, nrow = 1)

#Run a ratio between AB37 and AB42 loads
abratio <- ggplot(PL, aes(AB42Loads,AB37Loads))+geom_point(size=2.5)+labs(x = 'AB42 GM Load', y = 'AB37 GM Load', title = 'AB 37:42 ratio Load')+geom_smooth(method = "lm",se = FALSE)

#Run a sr of this
abratio <- ggscatter(PL, x = "AB42Loads", y = "AB37Loads",
                     add = "reg.line",  # Add regression line
                     add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                     conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank
abratio + stat_cor(method = "spearman", label.x = 600, label.y = -100)

#Run a ratio between ab37 plasma and ab42 plasma
ab2ratio <- ggplot(PL, aes(ab42plasma,ab37plasma))+geom_point(size=2.5)+labs(x = 'AB42 plasma', y = 'AB37 plasma', title = 'AB 37/42 plasma ratio')+geom_smooth(method = "lm",se = FALSE)

#Run a sr of this
ab2ratio <- ggscatter(PL, x = "ab42 plasma", y = "ab37plasma",
                      add = "reg.line",  # Add regression line
                      add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                      conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank
ab2ratio + stat_cor(method = "spearman", label.x = 70, label.y = 400)

ab2ratio + scale_y_log10()

#Scatterplot of 37/42 ratio of plasma vs loads
pload <- ggplot(PL, aes(AB37to42plasma,AB37to42Loads))+geom_point(size=2.5)+labs(x = 'AB37/42 Loads ratio', y = 'AB37/42 Plasma ratio', title = 'AB 37/42 plasma vs load ratio')+geom_smooth(method = "lm",se = FALSE)


#Add a Spearman rank of 37/42 ratio of plasma vs loads
pload <- ggscatter(PL, x = "37to42Plasma", y = "37to42Loads",
                   add = "reg.line",  # Add regression line
                   add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                   conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank for 37/42 ratio of plasma vs loads
pload  + stat_cor(method = "spearman", label.x = 1, label.y = 1)+
  scale_x_continuous(trans='log2')


#Scatterplot of 37/40 ratio of plasma vs loads
p2load <- ggplot(PL, aes(AB37to40plasma,AB37to40Loads))+geom_point(size=2.5)+labs(x = 'AB37/40 Loads ratio', y = 'AB37/40 Plasma ratio', title = 'AB 37/40 plasma vs load ratio')+geom_smooth(method = "lm",se = FALSE)


#Add a Spearman rank of 37/40 ratio of plasma vs loads
p2load <- ggscatter(PL, x = "37to40Plasma", y = "37to40Loads",
                   add = "reg.line",  # Add regression line
                   add.params = list(color = "blue", fill = "lightgray"), # Customize reg. line
                   conf.int = TRUE # Add confidence interval
)

#Add a Spearman Rank for 37/40 ratio of plasma vs loads
p2load  + stat_cor(method = "spearman", label.x = 0.04, label.y = 0.5)


#Regression analysis
PLs <- lm(BT2Loads~nt1plasma, data=PL)

plot(PL$BT2Loads, rstandard(PLs), ylab='Standardized Residuals', xlab='y') 
abline(h=0)

#fit robust regression model
robust <- rlm(BT2Loads~nt1plasma, data=PL)

#find residual standard error of PLs model. the lower the value for RSE, the more closely a model is able to fit the data.
summary(PLs)$sigma

#find residual standard errof of PLs model
summary(robust)$sigma

#robust regression analysis. data = dataset import. For example, when assigning PL <- read.csv file. data = PL 

library(robustbase) 
res <- lmrob(BT2Loads ~ nt1plasma,
             data=PL)
summary(res)
cbind(coef(res),confint(res, level = 0.95))

#This code tells R to run a linear regression using the lm function. The left side of the “~” symbol specifies the dependent variable; the right side specifies the independent variables. 
#The results from this regression analysis is then printed out using the summary function. Lastly, the cbind, coef and confint functions are used to print the model coefficients with the corresponding 95% confidence intervals.

#"Chan, G. and StatsNotebook Team (2020). StatsNotebook. (Version 0.1.0) [Computer Software]. Retrieved from https://www.statsnotebook.io"
#"R Core Team (2020). The R Project for Statistical Computing. [Computer software]. Retrieved from https://r-project.org"

res <- lm(BT2Loads ~ nt1plasma,
             data=PL)
summary(res)
cbind(coef(res),confint(res, level = 0.95))

library(car) #Linear Mixed Model Diagnostic. To check three assumptions (i.e. Linearity, homoscedasticity and normality) and perform model diagnostic, we will need to use several functions from the car library.

res.std <- rstandard(res)
plot(res.std, ylab="Standardized Residuals")

#The following lines of codes allow us to examine if there are outliers and if there are any observations that might have a large influence on the model estimates.
"Outlier Test. Observations with a Bonferroni p < .01 might be considered as outliers and might need further investigation."
outlierTest(res)
infIndexPlot(res)

"Residual plots, curvature tests and normality plot"
residualPlots(res)
ggplot(as.data.frame(res.std), aes(sample = res.std)) +
  geom_qq() +
  geom_qq_line()

"Variance inflation factor (VIF >=5 indicates high level of multicollinearity)"
vif(res)


#redone comparing all groups
res <- lm(nt1plasma ~ BT2Loads + AB40Loads + AB42Loads +AB37Loads
          data=PL)
summary(res)
cbind(coef(res),confint(res, level = 0.95))

#Generate a residual plot using the following lines of code. Points randomly scattered around 0. 
#Approximately 9% of the points will fall between -2 and 2 standardized residuals. Points larger may be outliers.
res.std <- rstandard(res)
plot(res.std, ylab="Standardized Residuals")

#The following lines of codes allow us to examine if there are outliers and if there are any observations that might have a large influence on the model estimates.
"Outlier Test. Observations with a Bonferroni p < .01 might be considered as outliers and might need further investigation."
outlierTest(res)
infIndexPlot(res)
#infIndexPlot. Observations with a Cook’s distance close to 1 indicates a high impact on the model coefficients.

"Residual plots, curvature tests and normality plot"
residualPlots(res)
ggplot(as.data.frame(res.std), aes(sample = res.std)) +
  geom_qq() +
  geom_qq_line()

"Variance inflation factor (VIF >=5 indicates high level of multicollinearity)"
vif(res)

#Calculating Interbatch Analysis for the different tau stainings


#Calculate mean for the given dataset, na.rm = true indicates removing any cells with missing information.
mean(PL$BT2Loads, na.rm = TRUE)
mean(PL$BT2LoadsRpt, na.rm = TRUE)
mean(PL$AT8Loads, na.rm = TRUE)
mean(PL$AT8LoadsRpt, na.rm = TRUE)

#Calculate standard deviation for the given dataset, na.rm = true indicates removing any cells with missing information.
sd(PL$BT2Loads, na.rm = TRUE)
sd(PL$BT2LoadsRpt, na.rm = TRUE)
sd(PL$AT8Loads, na.rm = TRUE)
sd(PL$AT8LoadsRpt, na.rm = TRUE)

#create a summary of the mean, media, and quartiles
summary(PL$BT2Loads)
summary(PL$BT2LoadsRpt)
summary(PL$AT8Loads)
summary(PL$AT8LoadsRpt)

#Measuring variability in a sample using variance.
var(PL$BT2Loads, na.rm = TRUE)
var(PL$BT2LoadsRpt, na.rm = TRUE)
var(PL$AT8Loads, na.rm = TRUE)
var(PL$AT8LoadsRpt, na.rm = TRUE)

#Measuring the coeffiecent of variation can be constructed using the previous data.
100 * sd(PL$BT2Loads, na.rm = TRUE) / mean(PL$BT2Loads, na.rm = TRUE) 
100 * sd(PL$BT2LoadsRpt, na.rm = TRUE) / mean(PL$BT2LoadsRpt, na.rm = TRUE) 
100 * sd(PL$AT8Loads, na.rm = TRUE) / mean(PL$AT8Loads, na.rm = TRUE) 
100 * sd(PL$AT8LoadsRpt, na.rm = TRUE) / mean(PL$AT8LoadsRpt, na.rm = TRUE) 

#Measuring the confidence intervals of the mean can be performed along with a t.test
t.test(PL$BT2Loads)$conf.int
t.test(PL$BT2LoadsRpt)$conf.int
t.test(PL$AT8Loads)$conf.int
t.test(PL$AT8LoadsRpt)$conf.int
